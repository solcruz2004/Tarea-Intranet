# URL base del servidor OpenAI-compatible de LM Studio
LM_STUDIO_BASE_URL=http://host.docker.internal:1234/v1

# Modelo expuesto por LM Studio
LM_STUDIO_MODEL=Meta-Llama-3-8B-Instruct

# Tamaño del modelo de Whisper a utilizar (tiny, base, small, medium, large-v2)
WHISPER_MODEL_SIZE=small

# Tipo de cómputo para faster-whisper (auto, int8, float16, etc.)
WHISPER_COMPUTE_TYPE=auto

# Idioma forzado para la transcripción (por ejemplo "es" para español).
# Si se deja vacío se detectará automáticamente.
WHISPER_LANGUAGE=

# Carpeta donde se guardarán las notas en el contenedor (se mapea desde docker-compose)
NOTES_ROOT=/app/notes

# Permite que la aplicación arranque los servicios auxiliares sin intervención
AUTO_BOOTSTRAP_SERVICES=true

# Abre automáticamente Obsidian cuando las notas están listas
AUTO_OPEN_OBSIDIAN=true

# Tiempo máximo (segundos) para esperar a que LM Studio y Docker estén listos
BOOTSTRAP_TIMEOUT_SECONDS=120

# Ruta al archivo docker-compose incluido en el paquete
DOCKER_COMPOSE_FILE=docker-compose.yml

# Comando alternativo para docker compose (por ejemplo "docker-compose")
DOCKER_COMPOSE_COMMAND=

# Comando para lanzar LM Studio en modo servidor (se completa en la distribución final)
LM_STUDIO_START_COMMAND=
LM_STUDIO_WORKDIR=

# Ubicación del vault de Obsidian y ejecutable portable
OBSIDIAN_VAULT=notes
OBSIDIAN_EXECUTABLE=

# Tema visual para la interfaz (consulta la documentación de ttkbootstrap)
GUI_THEME=superhero
